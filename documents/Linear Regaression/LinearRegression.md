# LinearRegression
LinearRegression(線型回帰)とは、最も単純な予測、分析モデルの一つです。
## Model
- $\bm{y}$ : 被説明変数(予測したい変数)
- $X$ : 説明変数(特徴量)
- $W, \bm{b}$ : パラメータ
- $\epsilon$ : ノイズ(残差ベクトル)
としたとき、以下のように書き下される。
```math
\bm{y} = WX + \bm{b} + \epsilon
```
これを、要素ごとに書くと、以下のようになる。
```math
y_i = \sum_j w_{i,j}x_{i,j} + b_i + \epsilon_i
```
つまり、被説明変数は、各説明変数による影響の和で表せるというモデルである。

損失関数は以下のように表され、最小二乗法と呼ばれる。
```math
L = \|\|\bm{y} - (WX + \bm{b})\|\|^2
```

これは、$\epsilon$が正規分布であると仮定すると、最尤推定となる。

## Algorithm
大域的最適階が解析的に求まる。

損失関数を微分すると、
```math
aa
```
